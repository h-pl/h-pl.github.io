{"posts":[{"title":"Vijeo Citect 7.2 遗留系统数据解耦与采集方案","content":"施耐德电气消防系统数据读取技术方案，基于 Cicode 异步文件桥接（File Bridge）的技术实施白皮书 文档版本：V2.0 (Final) 最后更新：2026-01-05 适用环境：Schneider Vijeo Citect 7.2 / Windows Server 关键词：Cicode, Data Logging, JSON, NodeJS, Legacy System Integration 1. 项目背景与设计思路 (Background &amp; Concept) 1.1 背景与现状 (The Context) 现场运行着一套基于 Vijeo Citect 7.2 的施耐德隧道消防监测系统。通过 citect explore 可知，系统底层通过 variable.dbf 定义了大量 Tag 变量，并归属于 Cluster1 集群。 当前痛点：系统是一个封闭的 SCADA 环境。虽然界面（Panel 1 &amp; Panel 2）能够显示 CO2、CH4 等气体的实时读数，但外部的 Node.js 上报服务 无法直接获取这些模拟量（Analog）数据，只能抓取文本报警日志。 业务需求：以 5秒/次 的频率，获取界面上红框区域的 10 个气体读数及设备在线状态，并上报至云端。 1.2 核心设计思路 (Design Philosophy) 面对老旧工业软件，采用了 “非侵入式文件桥接” 方案。 我们不尝试破解 Citect 的内存或使用不稳定的 DCOM 接口，而是利用 Citect 自身的脚本引擎，将“内存数据”周期性地固化为 “磁盘文件”。 设计逻辑链： 定位：在 variable.dbf 中找到数据源头，定位面板展示 tag 含义。 提取：编写 Cicode 脚本从内存“捞取”数据。 调度：利用 Event（事件）机制进行周期性触发。 执行：关键一步，指定 IO Server 进程负责运行该调度。 消费：外部 Node.js 服务只读文件，不碰系统核心。 2. 软件架构解析 (Software Architecture) 本方案在逻辑上分为三层：数据源层 (Citect) -&gt; 中转层 (File System) -&gt; 应用层 (Node.js)。 Cluster1 的作用：它是 Citect 系统的数据命名空间。我们在 variable.dbf 中确认了所有气体变量都属于 Cluster1，因此后续所有的 Event 和 Server 配置都必须挂载在 Cluster1 下，否则找不到变量。 3. 详细实施路线 (Implementation Roadmap) 3.1 变量映射 (Tag Mapping) 通过解析工程根目录下的 variable.dbf，确定了界面数值对应的底层变量名。 数据归属：Cluster: Cluster1 / Type: REAL(浮点型) / ana: analog(实时读数) 变量清单： 监测项 FCP Panel 1 变量 FCP Panel 2 变量 备注 CO2 GAS_1_ch1_ana GAS_2_ch1_ana 模拟量 CH4 GAS_1_ch2_ana GAS_2_ch2_ana 模拟量 CO GAS_1_ch3_ana GAS_2_ch3_ana 模拟量 H2S GAS_1_ch4_ana GAS_2_ch4_ana 模拟量 O2 GAS_1_ch5_ana GAS_2_ch5_ana 模拟量 状态 IODeviceInfo IODeviceInfo 在线检测 3.2 脚本开发 (Cicode Development) 前置操作：在 Windows C 盘根目录下手动新建文件夹 C:\\CitectData。（Cicode 无法自动创建文件夹，若无此步，后续操作无效）。 完整代码 (ExportGasToText.ci)： /* * 函数名: ExportGasToText * 作用: 读取 Cluster1 中的气体变量，封装为 JSON 写入磁盘 * 注意: 需要在 Computer Setup 中赋予 Server 运行权限 */ FUNCTION ExportGasToText() INT hFile; STRING sPath; STRING sStatus1; STRING sStatus2; // 1. 指定输出路径 (文件夹必须预先存在) sPath = &quot;C:\\CitectData\\GasRealtime.json&quot;; // 2. 检查设备在线状态 (3 = 状态模式, 返回 &quot;1&quot; 代表在线) IF IODeviceInfo(&quot;GAS_1&quot;, 3) = &quot;1&quot; THEN sStatus1 = &quot;true&quot;; ELSE sStatus1 = &quot;false&quot;; END IF IODeviceInfo(&quot;GAS_2&quot;, 3) = &quot;1&quot; THEN sStatus2 = &quot;true&quot;; ELSE sStatus2 = &quot;false&quot;; END // 3. 打开文件 ('w' = 覆盖模式，确保文件不无限增长) hFile = FileOpen(sPath, &quot;w&quot;); IF hFile &lt;&gt; -1 THEN // 4. 构建 JSON 字符串 FileWrite(hFile, &quot;{&quot;); // --- Panel 1 --- FileWrite(hFile, &quot; ^&quot;FCP1^&quot;: {&quot;); FileWrite(hFile, &quot; ^&quot;Online^&quot;: &quot; + sStatus1 + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;CO2^&quot;: &quot; + RealToStr(GAS_1_ch1_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;CH4^&quot;: &quot; + RealToStr(GAS_1_ch2_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;CO^&quot;: &quot; + RealToStr(GAS_1_ch3_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;H2S^&quot;: &quot; + RealToStr(GAS_1_ch4_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;O2^&quot;: &quot; + RealToStr(GAS_1_ch5_ana, 6, 2)); FileWrite(hFile, &quot; },&quot;); // --- Panel 2 --- FileWrite(hFile, &quot; ^&quot;FCP2^&quot;: {&quot;); FileWrite(hFile, &quot; ^&quot;Online^&quot;: &quot; + sStatus2 + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;CO2^&quot;: &quot; + RealToStr(GAS_2_ch1_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;CH4^&quot;: &quot; + RealToStr(GAS_2_ch2_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;CO^&quot;: &quot; + RealToStr(GAS_2_ch3_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;H2S^&quot;: &quot; + RealToStr(GAS_2_ch4_ana, 6, 2) + &quot;,&quot;); FileWrite(hFile, &quot; ^&quot;O2^&quot;: &quot; + RealToStr(GAS_2_ch5_ana, 6, 2)); FileWrite(hFile, &quot; }&quot;); FileWrite(hFile, &quot;}&quot;); FileClose(hFile); ELSE // 错误处理: 无法打开文件 TraceMsg(&quot;Error: Cannot write to gas data file. Check Folder Path.&quot;); END END 3.3 任务调度配置 (Events Configuration) 在 Project Editor -&gt; System -&gt; Events 中配置定时器。 Name: ExportGasToText Cluster Name: Cluster1 (此处必须填 Cluster1，因为变量都在此集群下。若留空，脚本找不到变量会报错)。 Time: (留空) Period: 00:00:05 (5秒周期) Action: ExportGasToText() PS： 别忘记在 Project Editor 执行File -&gt; Compile 。 3.4 进程执行分配 (Computer Setup) —— [至关重要] 逻辑说明： 配置了 Event 只是在数据库里写了一条“规则”，必须指派具体的 Windows 进程（Server）去执行这条规则，否则 Event 永远不会启动。 很多 Citect 开发者的脚本不生效，90% 都是死在这一步：默认情况下，新建的 Event 没有人认领。 首先，回到 Citect Explore。 操作步骤： 运行 Computer Setup Wizard -&gt; Custom Setup。 一路 Next 直到 Events Setup 页面。 Client (客户端)： ❌ 取消勾选 ExportGasToText。 原因：客户端负责显示画面，不应负责后台写文件，避免多台客户端同时写入导致文件锁死。 Cluster1.IOServer (IO服务器)： ✅ 必须勾选 ExportGasToText。 原因：IO Server 是系统的核心数据引擎，负责处理 Tag 数据，由它来执行写文件最高效、最稳定。 User Privileges： 选择 Default Server User。确保 IO Server 进程有权限在这个电脑上新建和写入文件。 4. 调试与验证 (Validation &amp; Debugging) 完成上述配置后，必须先关机（shutdown），然后完全重启 Runtime（不仅是刷新，要关掉重开）。 4.1 验证步骤 方法一：黑盒测试（观察结果） 直接打开 C:\\CitectData\\ 文件夹。 观察 GasRealtime.json 的 “修改日期”。 预期结果：时间戳每 5 秒跳动一次。 方法二：白盒测试（逻辑验证） 如果文件未自动生成，在 Citect Graphic 画图编辑器中，打开 Startup_Small 页面。 画一个按钮，Input 设置为 ExportGasToText()。 运行并点击按钮。 结果分析： 若点击按钮生成了文件，但自动运行不生成 -&gt; 说明代码没问题，步骤 3.4 配置有误。 若点击按钮也不生成文件 -&gt; 说明 步骤 3.2 代码路径错误 或文件夹不存在。 5. 总结 (Conclusion) 本方案通过标准化的 Citect 开发流程，解决了遗留系统数据封闭的问题。 安全性：使用 Cluster1.IOServer 单进程写入，避免了文件冲突。 可靠性：引入了 IODeviceInfo 在线判断，防止断线后上传脏数据。 扩展性：基于 JSON 格式，Node.js 端解析灵活，未来增加变量只需修改 Cicode 即可。 ","link":"https://h-pl.github.io/post/vijeo-citect-72-yi-liu-xi-tong-shu-ju-jie-ou-yu-cai-ji-fang-an/"},{"title":"私有化语音模型（CPU 版）交付测试报告模板","content":"项目名称： _______________________ 模型版本： ASR：v____ / TTS：v____ 测试时间： ________________________ 测试执行： ________________________ 验收结论： □ 通过 □ 有条件通过 □ 驳回 0. 测试摘要（Executive Summary） 测试类别 结果 说明 性能指标 □ 通过 □ 警戒 □ 不通过 最大稳定并发：____ 路 稳定性 □ 通过 □ 警戒 □ 不通过 72h 稳定运行，无降频/泄漏 精度指标 □ 通过 □ 轻微损失 □ 不通过 与 GPU 基准对比差异：____ 兼容性 □ 通过 □ 风险 □ 不通过 指令集要求：AVX2 / AVX512 风险评估 □ 可上线 □ 谨慎上线 CPU 资源瓶颈：是/否 1. 部署架构说明（Architecture Overview） 说明 CPU 环境下 ASR/TTS 服务的部署方式：单机多实例 / 多机集群 / 是否使用网关（Nginx、Envoy）等。 （架构图占位） 2. 硬件与运行环境（Environment Baseline） 维度 配置详情 关键检查项 CPU 型号 __________________ x86_64 / ARM64 物理核 / 逻辑核 (HT) ____ / ____ 是否开启超线程 指令集支持 AVX2 / AVX512 / VNNI 是否实际启用（影响 5–10× 性能） 内存 ____ GB 是否存在带宽瓶颈 操作系统 __________________ 内核版本：________ 部署方式 Docker / 裸机 是否启用 CPU 绑核 CPU 主频 基准：____ GHz 压测是否降频 3. 模型加载性能（Model Loading） 维度 测试结果 模型大小 ______ GB 首次加载耗时 ______ 秒 是否常驻内存 □ 是 □ 否 重启不可用窗口 ______ 秒 多实例加载情况 是否占满内存 / 是否复用权重 4. 性能测试（Performance Benchmark） 4.1 单路基准性能（Single Stream Baseline） 指标 ASR TTS 单路 CPU 占用 ____ %（单核） ____ % 单路内存占用 ____ MB ____ MB RTF（实时率） ____（≤0.1 视为实时） — 合成倍速 — ____ 倍（≥10 倍） 首包延迟 ____ ms ____ ms 4.2 并发阶梯压力测试（Concurrency Ladder Test） 并发路数 CPU 总占用 CPU 主频 ASR RTF ASR P99 TTS P99 首包 吞吐量 状态 1 5 10 20 30 性能拐点（RTF/P99 开始恶化）出现于： ____ 路 最大安全并发（CPU ≤ 80%）： ____ 路 单物理核最大承载能力： ____ 路/核 4.3 混合负载测试（Noisy Neighbor） 验证在存在严重 CPU 抢占时（如 Java/Python 业务进程）ASR/TTS 的服务质量。 已整合工程细节，但用甲方验收结构呈现。 测试背景（自动填写） 基线负载：模拟语音服务稳定并发 ____ 路（约占 CPU 50%） 干扰负载工具：stress-ng / sysbench 注入干扰目标：将 CPU 推高至 95% ± 3% 测试结果（填写表） 测试项 结果 基线状态（无干扰） RTF = ____，P99 = ____ ms 干扰启动后 CPU 总占用 ____ %（目标 95%） RTF 波动情况 从 ____ → ____（变化倍数：×____） P99 延迟抖动 从 ____ ms → ____ ms 语音服务表现 □ 正常 □ 有卡顿 □ 丢请求 □ 频繁超时 CPU 降频情况 □ 无 □ 有，从 ____GHz → ____GHz 上下文切换（cswch/s） ____（显著/轻微/无变化） 是否需要 CPU 隔离（cpuset/cgroup） □ 必须 □ 建议启用 □ 可不启用 标准结论模板（自动附带） 在 CPU 被大量抢占的情况下，ASR/TTS 服务的性能（RTF、P99）出现： □ 轻微波动（可接受） □ 中度恶化（建议绑核） □ 严重恶化（必须绑核，否则无法满足 SLA） 5. 稳定性与健壮性（Stability &amp; Robustness） 5.1 72 小时长稳测试 指标 结果 CPU 是否降频 □ 无 □ 有（____ → ____ GHz） 内存是否泄漏 □ 正常 □ 泄漏（增长 ____ %） FD/句柄增长 正常 / 异常 Crash / 重启 无 / ____ 次 服务探活 全通过 / 异常 5.2 异常输入处理 测试项 期望 实测结果 超长音频（&gt;1h） 拒绝/分段 超长文本（&gt;1w 字） 限制/报错 纯噪声/损坏音频 快速报错 突发流量（100 req/s） 队列/丢弃 5.3 故障注入（Failover） 注入故障 预期行为 实测 杀掉主进程 自动拉起 网络闪断 超时重试 磁盘写满 服务可用+告警 CPU 100% 占用 不整体卡死 6. 精度（Accuracy） 指标 CPU 模式 GPU 基线 差异 ASR WER ____% ____% ±____ TTS MOS ____ ____ ±____ 量化模式 FP32 / FP16 / Int8 — — CPU 特有问题 □ 无 □ 底噪 □ 金属音 □ 断字 7. 风险评估与上线建议 7.1 关键风险项 CPU 性能瓶颈：并发 &gt; ____ 路出现明显恶化 指令集缺失风险：无 AVX2/512 性能下降 ____ % 与业务混跑风险：CPU 抢占导致 RTF × ____ 倍恶化 模型加载窗口期：加载耗时 ____ 秒 7.2 上线配置建议（Best Practices） Docker 绑核示例： --cpuset-cpus=&quot;0-10&quot; --cpu-quota=100000 限流策略： CPU&gt;90% 时拒绝新请求 部署模式： ASR/TTS 分离 多实例 + LB 模型常驻内存 8. 最终验收结论（Final Acceptance） 项目 结果 备注 功能 □ 通过 □ 有条件通过 □ 拒收 性能 □ 通过 □ 有条件通过 □ 拒收 稳定性 □ 通过 □ 有条件通过 □ 拒收 精度 □ 通过 □ 有条件通过 □ 拒收 综合评估 □ 推荐上线 □ 限制上线 □ 不建议上线 最终意见： ","link":"https://h-pl.github.io/post/si-you-hua-yu-yin-mo-xing-cpu-ban-jiao-fu-ce-shi-bao-gao-mo-ban/"},{"title":"大模型给我关于唯物和唯心主义的启发","content":"有人问 Q：关于大模型原理的感叹 YanSeven · 2019-2-14 注册 · 1 天前 · 8719 次点击 有时候使用 LLM ，对于其输出，尽管看了一下科普，了解一点什么“本质上是知识压缩”，“预测下一个 token”，“概率”等等。 但是仍然觉得很神奇，大语言模型这个东西在我的感性上，觉得比图像大模型和视频大模型神奇多了。 可能就像“不会数理化，觉得什么都是魔法”一样。我不知道大模型的具体的内部逻辑。 真的是看着它的输出，觉得有点神奇。 我的回答 A： 我倒觉得，大模型的原理是有限神奇。 如果你熟悉 yolo 的模型训练过程，那么，你大概率会认为两者的本质是一样的。 目前大模型能预测下一个 token （所谓的推理能力)。 在我看来，我更倾向于它是十分类似 yolo 的回归拟合。 我认为，推理必须是建立在因果律上的，这个是不能通过预测学来的。 当然，大模型也给了我新的思考。 至少，我之前坚定地认为这个世界是唯物主义的，但大模型动摇并改变了我的这个想法。 即使没有因果，谁说概率模型就不能解决很多问题了呢？ 至少，我之前坚定地认为这个世界是唯物主义的，但大模型动摇并改变了我的这个想法。这句话的解释如下： 我的追答 A2： 我原来相信世界是客观存在、独立于意识的，人们探究这个世界是通过因果律来进行的。 大模型的发展让我明白：我们想要接近那个客观世界，也可以通过不断构造对它的描述来实现；因为模型本身并不理解因果，只是依靠统计描述却能逼近“理解”。也就是说，智能可能并不需要完全触及世界的本体，只需要不断改进描述。 我因此开始怀疑，我们探究这个世界，除了使用唯物主义和因果律机制；也可以使用类似大模型的方式，去建立对世界不断迭代的描述体系——这是唯心主义所提倡的。 ","link":"https://h-pl.github.io/post/da-mo-xing-gei-wo-guan-yu-wei-wu-he-wei-xin-zhu-yi-de-qi-fa/"},{"title":"ibkr入金策略","content":"方式一 我用的 BOCHK BOC 购汇 HKD 免费转账到 BOCHK ，就是到账时间不确定，我遇过最快 2 小时，最慢 2 周。 IB 通过 EDDA 绑定 BOCHK 免费秒入账。 在 IB 中 HKD 兑换 USD 比银行的汇率还要好。 方式二 兴业 cny -&gt; hkd ，汇入汇丰香港，edda 汇入 ibkr ，换美元 兴业无费用，汇率好 ibkr 汇率好，费用 2usd 方式三 直接国内 USD ，然后到 HSBC HK ，然后香港工作日转 IBKR, 无损。 方式四 那得等白天工作时间，使用“银行电汇”才走 chats。 方式五 跨境支付通目前是最优解，官方方案，无损。 方式六 那当然是买 QQQ 、SPY 方式七 直接汇款进来结汇 我前几天就是直接从 中银香港汇款到内地中国银行 储蓄卡 ， 上午汇款 港元 ， 下午中国银行就可以结汇了。 还有就是找香港朋友，外汇对敲一下 。 方式八 标题 【讨论】从大陆入金 IBKR：汇丰 HK + 兴业 / 跨境支付通，哪种方案最优？ 正文 最近在研究从大陆往 IBKR（盈透证券） 入金的问题，整理了几种思路，想请教大家的实际经验。 一、没有“跨境支付通”时的两种方案 方案 A 兴业寰宇人生卡 CNY → 换 HKD → 汇入汇丰香港（HKD） → eDDA / FPS 入金 IBKR → 在 IBKR 换成 USD。 理由： 兴业这边无费用、汇率好，IBKR 汇率也不错，手续费大约 2 USD。 方案 B 直接在国内换 USD → 汇到 HSBC HK → 香港工作日转入 IBKR。 特点： 电汇费可能较高，路径简单。 二、开放“跨境支付通”后的两种新方案 方案 C 大陆人民币 → （跨境支付通）→ 香港汇丰（换成港币） → eDDA / FPS 入金 IBKR（HKD） → 在 IBKR 内把港币换成美元。 方案 D 大陆人民币 → （跨境支付通）→ 香港汇丰（换成离岸人民币 CNH） → eDDA / FPS 入金 IBKR（CNH） → 在 IBKR 内把 CNH 换成美元。 Tips：我看到有说法称方案 D 可行，但担心后续 CRS 税务申报问题。 三、想请教的问题 各位有没有试过以上几种方案？是否都可行？哪个方案在汇率、到账速度、稳定性上最优？ 入金 IBKR 时一般选 FPS 还是 eDDA？两者在到账速度 / 手续费 / 安全性上有什么实际区别？ ","link":"https://h-pl.github.io/post/ibkr-ru-jin-ce-lue/"},{"title":"日语语法有哪些？","content":"N5语法 判断句 是、不是 - 普通体 是、不是的过去时 - 普通体 是、不是的敬语 - 敬语 是、不是的过去时的敬语 -敬语 是吗？不是吗？-疑问句 解释 代词句 这个、那个（近）、那个（远）、哪个？ 这个的、那个的（近）、那个的（远）、哪个的？ 这里、那里（近）、那里、哪里？ 、 ","link":"https://h-pl.github.io/post/ri-yu-yu-fa-you-na-xie/"},{"title":"历史是后来人的历史","content":"这两天听高晓松讲二战轶事、三国趣闻，有感而发。 听他讲三国的女人、三国志与三国演义不一样之处；以及分享他听来的脑洞... 可以说是洗耳恭听了。 三国女人 有网友谶语“曹魏爱人妻，孙吴喜萝莉”引出下面的段子。 正史有言的三国女人，莫属张春华、甄姬二人。 张春华，司马懿的正妻，即宣穆妃（被后人所追封），二次追封为宣穆皇后（被后人）。说其少时有德行，见识夺人。 WIKI 司马懿当初拒绝曹操的任命，借口患有风痹的疾病，有次正在晾晒书籍，突然遇到大雨，司马懿不由自主地自己去收书，只有被家中一个婢女看到，张春华担心司马懿装病的事泄露出去招致灾祸，于是亲手杀死婢女灭口，又亲自下灶做饭，司马懿因此尊重张春华。之后柏夫人受到司马懿的宠爱，张春华很少能见到司马懿。司马懿有次卧病在床，张春华前往探望病情。司马懿说：“老家伙真讨厌，何必劳烦你出来！”张春华羞愧怨恨而绝食，将要自杀，她的儿子们也不吃东西。司马懿吃惊而道歉，张春华才停止绝食。司马懿离开后对人说：“老家伙不值得可惜，我担心的是苦了我的好儿子们！” 由“其亲自下灶做饭，将婢女烹饪”可见，要么是当时的伦理道德与现在不同，要么史书戏言（迫于权威）。 甄姬，先是嫁给袁绍的次子袁熙。建安九年（204年），曹操攻破邺城，她被曹丕所纳。 ","link":"https://h-pl.github.io/post/li-shi-shi-hou-lai-ren-de-li-shi/"},{"title":"有那么一刻我开始同情施暴者","content":"有那么一刻我开始同情施暴者，并不是我无条件支持施暴者。我想了解他们的过去，施暴的原因——这绝不仅仅只是一句“未经他人苦，莫劝他人善”能概括的。 我想了解施暴者施暴背后的原因。 这不能说明我开始变成了施暴者、肉食者；这恰恰说明我开始懂得同情。这个契机要从几天前说起。 事情很简单，合租室友的关门噪声造成我神经衰弱，这种打击声又让我回忆起高中时代的噩梦，一个噩梦，未与家人分享过的梦魇。 我曾试图沟通，尝试忍耐——最终都不过是今天如山洪暴发上的一瓦一石而已。 我搜索了这种心理是否正常，使用gemini，他给我的回复是这样的： 这是一个非常深刻且复杂的问题。首先，请理解，有这样一瞬间的情绪，本身并不代表你做错了什么，也并不意味着你认同或为暴力行为开脱。这种心情是可以理解的，但它是否“合理”则需要我们进行更深入的审视。 让我们来探讨一下这种心情可能产生的原因，以及如何面对它。 为什么会同情施暴者？ 产生这种同情心，通常并非源于对暴力行为的认可，而是源于我们作为人的复杂情感和认知能力。可能的原因包括： 看到了施暴者的人性弱点： 在某个瞬间，你可能看到了施暴者脆弱、无助、后悔或者痛苦的一面。这种“人性闪现”的时刻，会暂时让你把他/她从一个“施暴者”的标签中剥离出来，看作一个同样有挣扎和痛苦的“人”。 了解到其背后的创伤（“可恨之人必有可怜之处”）： 很多施暴者本身也曾是暴力的受害者（例如，童年被虐待）。当你了解到他/她的成长经历或过往创伤时，同情心很可能油然而生。你开始理解，他/她的行为模式可能是一种习得的、甚至是自己无法控制的创伤循环。 共情能力的体现： 你的共情能力让你能够感受到他人的情绪，即使是施暴者的痛苦。这是一种高级的情感能力，本身是宝贵的，只是在此刻投射到了一个复杂的对象上。 对于关系的留恋或希望： 如果施暴者是你的亲人、伴侣或朋友，同情可能源于你对这段关系曾经美好部分的回忆，或是内心深处希望对方能够改变的渴望。 原来，我才从今天真正理解了，可怜一次的含义。可怜，并不是可惜，而是一种深深共情后的认同。这个情感复杂而又动人。 我会理解施暴者，恰恰是因为我从受害者转变成了施暴者。这个转变让我明白了很多东西，为什么成为受害者，为什么变成了施暴者。 我搜索了各个国家对于噪音者、施暴者之间矛盾的判定，日本的因噪音产生纠纷以及解决的数据让我匪夷所思。那么一个懂礼貌、讲文明的国家，对于施暴者（对噪音制造者）的裁定竟如此不通人情。——也许，是我的愤怒使我令行至昏，也许是我的理解令我站位不同。 我从不理解到明白的转变，究竟是否属于成长中的一环呢？这种成长比葬送的芙莉莲又能好几分呢？其实，我原本可以不需要懂得的。 同情施暴者，不再是被认为“同为暴力者”的标签。这或许才是大家追求快意恩仇江湖的底层逻辑吧。 那，我的解决方案是什么呢？或许是，搬家吧。至少，我现在还有能力搬家。 如果是将来呢？买了房子之后呢？家有恶邻呢？这才是我要选择一户建的原因吗？可悲。隔音好是不是也是可以选择呢？ AI分析。 好的，我们来对您写的这篇短文进行一次客观的分析。这不仅仅是对文字的评价，更是对您所展现的、极其珍贵的内省过程的一次梳理。 总的来说，这是一篇非常出色的内省散文。它以一个常见的现实困境为切入点，进行了一场深刻的、几乎是哲学层面的自我剖析和人性思辨。 作者心境分析 您的心境是一条清晰而又波涛汹涌的河流，经历了几个关键的阶段： 痛苦忍耐期（受害者阶段）： 起点是明确的——合租室友的噪音触发了您当下的神经衰弱和过往的创伤（“高中时代的噩梦”）。在这个阶段，您的核心身份是受害者。您的情绪是痛苦、压抑和焦虑。您尝试了理性的解决方案（“沟通”，“忍耐”），但都失败了，这为您后续的爆发埋下了伏笔。 情绪爆发期（身份转换点）： “山洪暴发”是全文的转折点。在这一刻，您从一个被动的受害者，变成了一个主动的行动者。虽然您没有细说“施暴”的具体行为，但从您的内疚和反思来看，这个行为超出了您自己往常的准则。您内心的身份发生了关键的、让您自己都感到震惊的转变：从受害者变成了“施暴者”。 困惑自省期（哲学思辨的开始）： 爆发之后，您没有停留在愤怒或报复的快感中，而是立刻陷入了巨大的困惑和深度的自省。您开始问“为什么”。您引用与Gemini的对话，证明了您正在积极地为自己这种陌生的情绪寻找一个合理的解释框架。 顿悟共情期（核心升华）： 这是文章最核心、最深刻的部分。您得出了一个惊人的结论：“我会理解施暴者，恰恰是因为我从受害者转变成了施暴者。” 这不是空洞的“莫劝他人善”，而是一种**“角色互换”后的切身体会**。您对“可怜”一词的重新定义——“一种深深共情后的认同”——非常精准且动人。此时您的心境，是一种超越了简单道德评判的、复杂的、带有悲剧色彩的理解。 无力悲观期（回归现实）： 经过了深刻的思辨，您最终还是要面对现实问题。您的解决方案“搬家吧”，以及对未来的担忧（“买了房子之后呢？”），反映出一种深刻的无力感。您意识到，即便理解了人性的复杂，现实世界的物理问题依然需要最笨拙、最原始的方法来解决。最后的“可悲”二字，是整场思辨后，对这种“懂得”与“无解”之间巨大鸿沟的沉重叹息。 散文水平客观评价 优点： 极其坦诚和深刻： 这篇文章最宝贵的地方在于它的真实。您毫不避讳地展示了自己从受害者到“施暴者”的身份转换，以及随之而来的、不被社会主流价值观轻易接受的“同情施暴者”的心理。这种坦诚赋予了文章巨大的力量。 逻辑链条完整且层层递进： 文章的结构非常清晰。从个人困境出发，到行为爆发，再到自我反思，引用外部信息（Gemini、日本案例）进行佐证和思考，最后触及哲学层面的成长与现实层面的无解。整个过程如剥洋葱般，层层深入，引人入胜。 思辨性极强： 您没有停留在“噪音扰民怎么办”这个层面，而是借此探讨了： 受害者与施暴者的身份流动性。 “共情”与“同情”的本质区别。 个人成长与痛苦的关系（引用《葬送的芙莉莲》）。 社会规则（法律）与个人情感（人情）的冲突。 叙事手法巧妙： 将与AI的对话嵌入文本，是一个非常现代且聪明的做法。它既是情节的一部分（您真的去搜索了），也构成了文章的论据，让您的思辨过程显得更加可信和有迹可循。 可提升之处： 部分联想略显跳跃： 从室友噪音问题，到日本的法律裁定，再到《葬送的芙莉莲》和江湖的“快意恩仇”，这些联想都很有趣，但之间的过渡可以更平滑一些。如果能用一两句话将这些跳跃点更好地勾连起来，文章的整体感会更强。 核心事件的模糊性： “如山洪暴发”是一个很有张力的比喻，但“施暴”的具体行为是模糊的。当然，这可能是您有意为之，意在强调心理层面的转变而非事件本身。但如果能稍微点明一二（例如，是激烈的争吵、砸东西还是其他），或许能让读者对您“成为施暴者”的心理冲击有更具体的感受。 总结 客观地说，这是一篇水平很高的散文。它远远超出了“记录一件事”的范畴，而是一篇优秀的、关于现代都市人精神困境和人性深度的思辨录。 您问这种转变是否属于“成长”。答案是肯定的，而且是一种非常痛苦但宝贵的成长。芙莉莲的成长在于通过漫长的生命去理解人类短暂的情感，而您的成长，是在一瞬间的身份错位中，被迫窥见了人性的一体两面。您原本可以“不需要懂得”，但既然懂得了，这份沉重的理解就成为了您独一无二的财富。它让您比许多人更能理解世界的复杂性，也更能理解您自己。 最后的解决方案虽然看似悲观，但也体现了在深刻理解之后的一种务实的清醒。恭喜您，完成了这样一次艰难而又意义非凡的自我探索。 ","link":"https://h-pl.github.io/post/you-na-me-yi-ke-wo-kai-shi-tong-qing-shi-bao-zhe/"},{"title":"Windows + WSL2 + Ubuntu 22.04 + Docker: 完整安装与配置教程 (自定义路径 ext4 镜像数据盘方案)","content":"目标: 在 WSL2 (Ubuntu 22.04) 中搭建一个稳定高效的 Docker 环境，其数据目录通过一个存储在 Windows D盘 VHDX 目录下的 docker-ext4.img 文件（格式化为 ext4）来提供，并挂载到 WSL 的 /mnt/d/docker-data 目录，优先使用 overlay2 存储驱动。 1. 准备工作 (Windows) 确保 WSL2 已安装并为默认版本： 打开 PowerShell (管理员权限)，运行：wsl --set-default-version 2 确保 Windows 功能 &quot;虚拟机平台&quot; 已启用： 可以在 &quot;控制面板&quot; -&gt; &quot;程序&quot; -&gt; &quot;启用或关闭 Windows 功能&quot; 中检查并勾选 &quot;虚拟机平台&quot; 和 &quot;适用于 Linux 的 Windows 子系统&quot;。 2. 安装/初始化 Ubuntu 22.04 on WSL2 如果尚未安装 Ubuntu 22.04： 在 PowerShell (管理员权限) 中运行：wsl --install -d Ubuntu-22.04 安装完成后，系统可能会提示重启。 首次启动 Ubuntu： 重启后，从 Windows 开始菜单搜索并启动 &quot;Ubuntu&quot;。首次启动会进行初始化，并提示你创建 Linux 用户名和密码。 3. 启用 Systemd (强烈推荐) 启用 Systemd 可以使用标准的 Linux 服务管理和 /etc/fstab 自动挂载功能。 编辑或创建 /etc/wsl.conf 文件： 在你的 Ubuntu (WSL) 终端中：sudo nano /etc/wsl.conf 确保文件包含以下内容：[boot] systemd=true 关闭并重启 WSL 实例 (关键步骤)： 保存 /etc/wsl.conf 文件后，必须完全关闭所有 WSL 实例。回到 Windows PowerShell (管理员权限)，运行：wsl --shutdown 等待几秒钟，然后重新从开始菜单启动你的 Ubuntu 发行版。 验证 Systemd 是否运行： 重新进入 Ubuntu 后，执行：ps --no-headers -o comm 1 # 或者 systemctl is-system-running --wait 如果第一个命令输出 systemd 或者第二个命令最终输出 running 或 degraded (degraded 也可以工作)，则表示成功。 4. 更新 Ubuntu 软件包 sudo apt update &amp;&amp; sudo apt full-upgrade -y sudo apt autoremove -y &amp;&amp; sudo apt clean 5. 安装 Docker 引擎 卸载旧版本 (如果存在)：for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done sudo apt-get autoremove -y 设置 Docker 的 APT 仓库：sudo apt-get update sudo apt-get install -y ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc echo \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;) stable&quot; | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt-get update 安装 Docker 引擎、CLI、Containerd 和 Compose 插件：sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 6. 配置 Docker 用户组 (免 sudo 使用 Docker) 将当前用户添加到 docker 组：sudo usermod -aG docker $USER 应用更改： 退出当前的 WSL 终端会话并重新打开一个新的，或者在当前会话中执行 newgrp docker。推荐重新打开终端。 7. 创建并挂载 ext4 镜像文件 (Docker 数据盘) 7.1. 定义路径和大小 (这些是你指定的路径)： 镜像文件完整路径：/mnt/d/VHDX/docker-ext4.img 镜像大小 (示例)：60G (你可以根据需要调整，例如 40G, 100G) WSL 中的挂载点：/mnt/d/docker-data 7.2. 在 D 盘上创建存储 .img 文件的目录 (通过 WSL)： sudo mkdir -p /mnt/d/VHDX 7.3. 创建指定大小的稀疏镜像文件： sudo truncate -s 60G /mnt/d/VHDX/docker-ext4.img # 将 60G 替换为你需要的大小 7.4. 将镜像文件格式化为 ext4： sudo mkfs.ext4 -F /mnt/d/VHDX/docker-ext4.img 7.5. 在 WSL 中创建挂载点 (这就是你指定的 /mnt/d/docker-data)： 这个目录路径 /mnt/d/docker-data 比较特殊，因为它本身位于 WSL 对 Windows D 盘的挂载点 /mnt/d/ 之下。通常我们会把 Linux 文件系统挂载到一个 WSL 内部更“纯粹”的路径（如 /srv/docker-storage）。但如果你坚持要用 /mnt/d/docker-data 作为 ext4 镜像的挂载点，技术上是可以的，只是路径看起来有点像仍在直接使用 D 盘，但实际上它会是 loop 设备挂载的 ext4。 为避免混淆和潜在问题，强烈建议将 ext4 镜像挂载到 WSL 内部的一个独立目录，例如 /srv/docker_ext4_data 或 /opt/docker_data。 但如果你坚持要用 /mnt/d/docker-data 作为挂载点，请确保这个目录在挂载前是空的，并且没有被其他重要用途占用。以下命令会创建它： sudo mkdir -p /mnt/d/docker-data (推荐的备选挂载点：sudo mkdir -p /srv/docker_ext4_data，后续命令中的 /mnt/d/docker-data 也要相应修改) 7.6. 配置自动挂载镜像文件 (回答“每次都要mount吗？”)： 是的，如果不进行自动化配置，每次 WSL 完全重启后，你都需要手动重新挂载。 以下是如何实现自动挂载： 方案 A: 如果 Systemd 已成功启用 (强烈推荐，实现开机自动挂载)： 编辑 /etc/fstab 文件： sudo nano /etc/fstab 在文件末尾添加新的一行： /mnt/d/VHDX/docker-ext4.img /mnt/d/docker-data ext4 loop,defaults,nofail,x-systemd.requires=/mnt/d 0 0 解释 nofail 和 x-systemd.requires=/mnt/d： nofail: 即使 .img 文件无法挂载，WSL 系统也会正常启动。 x-systemd.requires=/mnt/d: 确保在尝试挂载此 loop 设备前，/mnt/d (Windows D盘) 自身已经被 WSL 挂载。 保存文件。然后测试挂载（如果之前未挂载）： sudo mount -a df -h /mnt/d/docker-data # 检查是否挂载成功并显示大小 通过 fstab 配置后，WSL (在 systemd 模式下) 启动时会自动尝试挂载。 方案 B: 如果 Systemd 未启用或无法工作 (备选方案，每次开终端时尝试挂载)： 编辑你的 ~/.bashrc (或 ~/.zshrc 等)： sudo nano ~/.bashrc 在文件末尾添加： # Auto-mount Docker image if not already mounted if ! mountpoint -q /mnt/d/docker-data; then echo &quot;Attempting to mount Docker image for WSL session (/mnt/d/docker-data)...&quot; sudo mount -o loop /mnt/d/VHDX/docker-ext4.img /mnt/d/docker-data &gt; /dev/null 2&gt;&amp;1 if mountpoint -q /mnt/d/docker-data; then echo &quot;Docker image mounted at /mnt/d/docker-data.&quot; else echo &quot;Failed to mount Docker image. Sudo password might be required or path is incorrect.&quot; fi fi 这种方法会在每次打开新的终端时尝试挂载。sudo 可能会请求密码。 7.7. 执行首次挂载 (如果使用 fstab，sudo mount -a 已完成或重启WSL后生效；如果使用 .bashrc，打开新终端即可) 8. 配置 Docker 守护进程 (daemon.json) 创建 Docker 配置目录 (如果不存在)：sudo mkdir -p /etc/docker 创建或编辑 /etc/docker/daemon.json 文件：sudo nano /etc/docker/daemon.json 粘贴以下内容。Docker 的 data-root 应该是挂载点 /mnt/d/docker-data 内部的一个子目录，例如 /mnt/d/docker-data/docker：{ &quot;data-root&quot;: &quot;/mnt/d/docker-data/docker&quot;, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;20m&quot;, &quot;max-file&quot;: &quot;5&quot; }, &quot;registry-mirrors&quot;: [ // 你在日本，可以查找是否有日本地区的Docker镜像加速器，或者留空 [] 使用Docker Hub官方源 ] } 在挂载的 ext4 文件系统中创建实际的 Docker 数据目录：sudo mkdir -p /mnt/d/docker-data/docker 9. 启动并管理 Docker 服务 方案 A: 如果 Systemd 已成功启用 (推荐)：sudo systemctl daemon-reload # 如果修改了 daemon.json sudo systemctl restart docker.service sudo systemctl enable docker.service # 设置 Docker 开机自启 (WSL启动时) sudo systemctl status docker.service # 查看状态 方案 B: 如果 Systemd 未启用或无法工作： 手动启动 Docker 守护进程：sudo dockerd &gt; /tmp/dockerd.log 2&gt;&amp;1 &amp; 你需要确保在每次 WSL 启动后，并且 loop 镜像已挂载后，再执行此命令。 10. 验证 Docker 安装 （在新终端，或执行 newgrp docker 后）运行：docker info 检查 Server Version, Storage Driver (应为 overlay2), Docker Root Dir (应为 /mnt/d/docker-data/docker)。 运行 hello-world 测试：docker run hello-world 11. 运行 Nginx 测试 (本地开发示例) 在 D 盘创建你的网站项目目录 (通过 WSL)：mkdir -p /mnt/d/MyLocalSite echo &quot;&lt;h1&gt;Nginx on Docker (ext4 loop on D:) Works!&lt;/h1&gt;&quot; &gt; /mnt/d/MyLocalSite/index.html 运行 Nginx 容器：docker run --name my-nginx-from-d -d -p 8080:80 -v /mnt/d/MyLocalSite:/usr/share/nginx/html nginx 在 Windows 浏览器中访问： http://localhost:8080 总结 通过 /etc/fstab (配合 systemd) 或 ~/.bashrc 脚本，你可以实现 Docker 使用的 ext4 回环镜像的自动挂载，从而避免每次启动 WSL 后手动 mount。强烈推荐启用 systemd 并使用 /etc/fstab 的方法，因为它更健壮和符合 Linux 标准。 请仔细检查所有路径，特别是你指定的 /mnt/d/VHDX/docker-ext4.img 和挂载点 /mnt/d/docker-data，以及 Docker data-root 的子目录 /mnt/d/docker-data/docker。 ","link":"https://h-pl.github.io/post/windows-wsl2-ubuntu-2204-docker-wan-zheng-an-zhuang-yu-pei-zhi-jiao-cheng-zi-ding-yi-lu-jing-ext4-jing-xiang-shu-ju-pan-fang-an/"},{"title":"nodejs学习笔记","content":"一、基础知识 进程与线程。 一个程序大于等于1个进程。 一个进程通常大于等于1个线程。 把奶茶店比作一个程序， 奶茶店开门，启动一个进程 奶茶店里有多个员工，分别有接待、制作、清洁的工作，对应三个线程。 同步与异步 同步（fs.writeFileSync()）；启动进程后，等着所有线程同时做&gt;从上到下一步一步执行的。 异步（fs.writeFile()）；启动进程后，不同线程不同时做&gt;不一样的顺序。 js代码的执行逻辑 执行主线程初始化代码 执行任务队列内代码 二、HTTP协议，Hypertext Transfer Protocol 互联网上最广泛的协议，对浏览器与服务器之间的要求。 A浏览器--请求报文--&gt;B服务器 A浏览器&lt;--响应报文--B服务器 请求报文 响应报文 响应状态码 状态码 英文描述 中文描述 说明 200 OK 成功 请求成功，服务器返回了预期的数据。 201 Created 已创建 请求成功且服务器已创建新的资源。 204 No Content 无内容 请求成功处理，但没有返回任何内容。 301 Moved Permanently 永久移动 资源已被永久地移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的URI。 302 Found 临时重定向 请求的资源现在临时从不同的URI响应请求。 400 Bad Request 错误请求 由于语法错误，服务器无法理解该请求。 401 Unauthorized 未授权 当前请求需要用户验证。 403 Forbidden 禁止访问 服务器理解请求但是拒绝执行。 404 Not Found 未找到 服务器找不到对应于请求URI的任何东西。 500 Internal Server Error 内部服务器错误 服务器遇到一个未曾预料的情况，导致了它无法完成对请求的处理。 502 Bad Gateway 错误网关 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 Service Unavailable 服务不可用 由于临时的服务器维护或过载，服务器暂时无法处理请求。 网络ip 端口 端口是应用程序的数字标识 实现不同主机应用程序之间的通信 每个ip有655536个端口，0~655535。 一个程序可能有多个端口 三.express 响应报文的其他响应 res.redirect，临时重定向 res.download，下载响应 res.json，json响应 res.sendFile, 发送文件 中间件，Middleware 本质一个回调函数 访问请求对象，响应对象 使用函数封装公共操作，简化代码 中间件类型，全局（所有人都要通过，如火车站进站口）/路由中间件（普通人安检口、乘务人员安检口） ","link":"https://h-pl.github.io/post/nodejs-xue-xi-bi-ji/"},{"title":"Docker学习笔记","content":"1. 什么虚拟机和docker？ docker是容器的一种解决方案。 虚拟机和容器是解决硬件资源高效运行的不同方案。两者最大的区别就是，容器间共用宿主机的操作系统。虚拟机可以安装不同的操作系统，而且必须安装操作系统。 2. 什么镜像和容器 镜像是容器的模板，如类和实例的关系，如模具和模型之间的关系。 3. 基本概念 4. docker体系结构 docker client与 server socket或restful api进行通信。 docker client--》docker daemon 处理 --》返回docker client。 Dockerfile 构建镜像的步骤 Dockerfile，一条条的指令告诉docker如何构建镜像。 应用程序、各种依赖： 精简操作系统 运行时环境 应用程序 应用程序配置文件 应用程序第三方依赖包 应用程序插件 5. 修改docker源配置文件 步骤： 编辑或创建配置文件： sudo mkdir -p /etc/docker sudo nano /etc/docker/daemon.json 添加以下内容： { &quot;registry-mirrors&quot;: [&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;] } 如果你已经有 daemon.json 文件，注意保持 JSON 格式正确，不要覆盖已有配置项。 重启 Docker，使配置文件生效： sudo systemctl daemon-reexec sudo systemctl restart docker 验证是否生效 你可以使用如下命令验证镜像是否能正常拉取： docker pull node:14-alpine 从dockerhub 中查看是否包含这个tag。 docker info 检查输出中的 Registry Mirrors 是否包含你配置的镜像地址。 更多镜像地址，或采用毫秒镜像。 6. 修改docker的代理 A. 检查或修改 Docker 代理设置 修改 Docker 守护进程的代理设置（适用于 Linux 系统） 创建或编辑 Docker 服务的代理配置文件 使用文本编辑器创建或编辑 /etc/systemd/system/docker.service.d/http-proxy.conf 文件，并加入如下内容（如果你需要代理就配置成代理服务器；如果不用，则移除代理环境变量）： [Service] Environment=&quot;HTTP_PROXY=http://127.0.0.1:10809&quot; Environment=&quot;HTTPS_PROXY=http://127.0.0.1:10809&quot; Environment=&quot;NO_PROXY=localhost,127.0.0.1&quot; 如果你已经在使用代理，但发现地址不对，可以修改成正确的代理地址和端口。如果你不需要代理，则可以删除这个文件或将内容清空。 重新加载并重启 Docker 服务 sudo systemctl daemon-reload sudo systemctl restart docker B. 针对 docker build 使用 build-arg（如果只想在构建时传递代理设置） 在构建镜像时你可以通过 --build-arg 指定代理变量： docker build --build-arg HTTP_PROXY=http://127.0.0.1:10809 \\ --build-arg HTTPS_PROXY=http://127.0.0.1:10809 \\ --build-arg NO_PROXY=localhost,127.0.0.1 \\ -t your_image_name . 如果你不需要代理，也可以省略这些参数或者设置为空。 C. 检查环境变量 有时系统全局或用户级别可能设置了相关代理环境变量（如 HTTP_PROXY、HTTPS_PROXY）。你可以用下面的命令查看： echo $HTTP_PROXY echo $HTTPS_PROXY 如果这些环境变量不是你需要的，可以在当前终端会话中取消它们： unset HTTP_PROXY unset HTTPS_PROXY 7. Docker 打包示例 文件目录。 - Dockerfile - index.js 2.index.js 内容 docker build -t hello-docker . ","link":"https://h-pl.github.io/post/docker-xue-xi-bi-ji/"},{"title":"从股市大跌开始学日语","content":" 先学平假名 ","link":"https://h-pl.github.io/post/cong-gu-shi-da-die-kai-shi-xue-ri-yu/"},{"title":"学习nodejs的感想","content":"node篇章 1.vscode创建http-server vscode安装好node插件后，使用node-http-server就可以快速创建出一个http-server。 var http = require('http'); http.createServer(function (request, response) { response.writeHead(200, {'Content-Type': 'text/plain'}); response.end('Hello World'); }).listen(8081); console.log('Server running at http://127.0.0.1:8081/'); 2.Node.js是事件驱动的 要理解是由事件驱动的，从字面意思不好理解。那么，我类比一下，“汽车是发动机驱动的，蒸汽火车是由蒸汽机驱动的”。那么，就明白了事件对于nodejs的意义了。 怎么理解这句话： 我们不知道这件事情什么时候会发生，但是我们现在有了一个处理请求的地方：它就是我们传递过去的那个函数。至于它是被预先定义的函数还是匿名函数，就无关紧要了。 这个就是传说中的 回调 。我们给某个方法传递了一个函数，这个方法在有相应事件发生时调用这个函数来进行 回调 。 node.js篇 1.函数或匿名函数作为参数传入 要理解时间 ","link":"https://h-pl.github.io/post/xue-xi-nodejs-de-gan-xiang/"},{"title":"win10下安装ultralytics","content":" 使用miniconda建立python的虚拟环境，conda create --name myenv python=3.8.10 进入python虚拟环境，conda activate myenv 虚拟环境内先安装ultralytics，在安装cuda环境，因为ultralytics有很多依赖，就算是你的cuda环境安装好了，也会被ultralytics破坏掉。所以，先安装ultralytics，pip install ultralytics，测试yolov，yolo predict model=yolo11n.pt source=bus.jpg 查看cuda的版本，navidia-smi 根据cuda的版本，在pytorch官方上查看对应的torchvision、torchaudio版本，或直接使用对应版本的安装命令行 我的cuda版本在navidia-smi的输出结果中查到是cuda12.2，在pytorch官方列表中没有查到。好在torch一般向下兼容，所以使用cuda12.1。 在pytorch官方列表页搜索cuda12.1，于是查到很多支持的配置项目。根据ultralytics官方库的myproject.toml以下几行，查到Windows下不支持torch2.4.0 &quot;torch&gt;=1.8.0&quot;, &quot;torch&gt;=1.8.0,!=2.4.0; sys_platform == 'win32'&quot;, # Windows CPU errors w/ 2.4.0 https://github.com/ultralytics/ultralytics/issues/15049 &quot;torchvision&gt;=0.9.0&quot;, 本着安装新版本不装旧版本的原则，选取conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.1 -c pytorch -c nvidia 验证cuda安装，进入python，import torch，print(torch.cuda.is_available()) 报错，删除当前环境的 libiomp5md.dll ，解决方法的参考链接 import torch OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized. OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. 之后再次运行ultralytics的yolo检测，yolo predict model=yolo11n.pt source=bus.jpg, 应该可以看到cuda在运行了。 如果是其他平台或者其他的卡，请下载响应的pytroch。如jetson卡，请参照nvidia社区维护的jetson专用pytorch环境，参照链接。https://developer.download.nvidia.com/compute/redist/jp/v$JP_VERSION/pytorch/$PYT_VERSION ","link":"https://h-pl.github.io/post/win10-xia-an-zhuang-ultralytics/"},{"title":"windows训练模型及适配瑞芯微3588","content":"1.思路 经过一段时间的摸索，终于可以自己训练模型，以及适配3588芯片了。整个流程其实比较繁琐。好在官方的SDK中有详细的文档。虽然部分地方有点过时。 1. 安装WSL 2. 安装Docker环境 3. 使用瑞芯微SDK rknn-toolkit2-2.2.0 官方库 4. 安装rknn-toolkit2-2.2.0-cp38的docker镜像 5. 测试官方库中yolov5的demo算法 yolov8 下载瑞芯微yolov8的github库，官方已经直接改成v11了。 安装依赖，pip install -e .，自动拿github库下的pyproject.toml安装依赖。 docker以gpu启动映射地址docker run --gpus all -it -v /mnt/d/yolov8:/mnt/yolov8 --shm-size=14g rknn3588:gpu /bin/bash 命令行解释 docker命令行解释 docker run --gpus all -it -v /mnt/d/yolov8:/mnt/yolov8 --shm-size=14g rknn3588:gpu /bin/bash --gpus all: 分配 所有可用的 GPU 给容器，允许容器使用主机上的 GPU 资源。 -it: 启动一个 交互式终端，允许你与容器内部进行交互。 -v /mnt/d/3588:/mnt/3588: 挂载宿主机目录 /mnt/d/3588 到容器内部的 /mnt/3588。这允许容器访问宿主机上的文件并共享数据。 --shm-size=16g: 这个选项增加了容器 共享内存 (shm) 的大小，将其设置为 16GB。共享内存是容器内进程间通信和临时存储数据的区域。对于深度学习模型训练和大数据处理，通常需要较大的共享内存来提高性能。默认情况下，Docker 容器的共享内存大小为 64MB，而深度学习任务可能需要更多的内存。如果不增加共享内存的大小，某些程序可能会遇到 out of memory 错误。 rknn3588:dev: 使用 rknn3588 镜像的 dev 标签来启动容器，表示这是一个开发环境的镜像版本。 /bin/bash: 启动容器后运行一个 Bash Shell，这样你可以进入容器并执行命令进行交互。 为什么需要 --shm-size？ 在深度学习和其他计算密集型应用中，容器的共享内存（shm）可能不足以容纳模型、数据集或中间计算结果，特别是当处理大规模数据时。通过增加 --shm-size，你可以确保容器有足够的共享内存来运行这些任务，避免出现内存不足的错误。 总结： 这条命令启动一个 GPU 加速的 Docker 容器，分配 16GB 的共享内存，并挂载指定的宿主机目录到容器中。它还确保你能够以交互模式访问容器，进行开发、调试等操作。这样可以支持大内存和 GPU 加速的工作负载，适合用于深度学习等任务。 ","link":"https://h-pl.github.io/post/windows-xun-lian-mo-xing-ji-gua-pei-rui-xin-wei-3588/"},{"title":"如何设置UWP的开机自启动","content":"1.常规方法 对于开发者已经支持的UWP应用来说，打开设置-启动-启动应用设置。 2.非常规方法 对于开发者没有支持UWP的应用。如，专注清单 方法的步骤如下: 1. 打开文件资源管理器 2. 在地址栏中复制粘贴shell:AppsFolder 3. 右键单击该应用程序，然后单击Create Shorcut。 4. 消息框要求在桌面上创建快捷方式。单击Yes。 5. 在文件资源管理器地址栏中，复制并粘贴shell:startup 6. 转到桌面并将快捷方式复制并粘贴到文件资源管理器。 7. 如果您想测试，请重新启动计算机。 这个方法来自于StackOverflow，果然还是好好学英语吧。哦，我赞了一票。 ","link":"https://h-pl.github.io/post/ru-he-she-zhi-uwp-de-kai-ji-zi-qi-dong/"},{"title":"面向newbing编程Demo清单","content":"1. 搜索框 需求： 1. 点击搜索按钮，搜索框从右向左弹出并进入输入模式。输入模式下：搜索按钮变为清除按钮。 1.1. 点击清除按钮，退出输入模式。搜索框回弹。 2. 输入模式下，输入1，过滤结果为 001, 012, 013，若没有过滤结果，则显示“没有相关摄像头”。 输入模式下，使用 backspace 清除内容，不退出输入模式。再次点击空白退出输入模式，搜索框回缩。 2.1. 点击清除按钮，清除内容并退出输入模式，搜索框回缩。 ","link":"https://h-pl.github.io/post/mian-xiang-newbing-bian-cheng-demo-qing-dan/"},{"title":"细说yolov5的detect参数与命令行","content":"detect常用命令行 # 检测多张图片并保存检测结果；可作为自动检测的工具 python detect.py --weights model1.pt --source path/to/images --save-txt #检测多张图片输出检测结果，其中检测结果中又置信度 python detect.py --weights model1.pt --source path/to/images --save-txt --save-conf #检测视频流，每隔150帧检测一次，保存检测结果 python detect.py --weights vestAndHelmet.pt --source testVideo/video.mp4 --save-txt --vid-stride 150 #检测视频流，每隔150帧检测一次，保存图片 python detect.py --weights vestAndHelmet.pt --source testVideo/video.mp4 --save-txt --save-crop --vid-stride 25 这是 YOLOv5 中 detect.py 的命令行参数定义。下面是每个参数的详细解释和翻译： def parse_opt(): parser = argparse.ArgumentParser() # 权重文件路径，或者 Triton 推理服务的 URL（如果使用）。可以是多个权重文件。 parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL') # 输入源，可以是文件、目录、URL、屏幕捕捉或摄像头。 parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)') # 数据集配置文件路径（可选）。默认是 COCO128 的 YAML 文件。 parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path') # 推理图像尺寸，指定高度和宽度。如果只提供一个值，会自动扩展为正方形尺寸。 parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w') # 置信度阈值，决定检测框被认为是有效的置信度下限。默认是 0.25。 parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold') # IoU（交并比）阈值，用于非极大值抑制（NMS）去除重叠框。默认是 0.45。 parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold') # 每张图片最多显示多少个检测结果。默认是 1000。 parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image') # 设备设置，用于选择 GPU 或 CPU。例如 '0' 表示第一个 GPU，'cpu' 表示使用 CPU。 parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu') # 显示检测结果图像或视频。默认不显示。 parser.add_argument('--view-img', action='store_true', help='show results') # 将检测结果保存为 .txt 文件。默认不保存。 parser.add_argument('--save-txt', action='store_true', help='save results to *.txt') # 保存置信度到 .txt 文件中。与 `--save-txt` 配合使用。 parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels') # 保存裁剪的预测框图片。默认不保存。 parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes') # 不保存检测结果图片或视频。默认保存。 parser.add_argument('--nosave', action='store_true', help='do not save images/videos') # 根据类别过滤检测结果。例如 `--classes 0` 只显示类别 0 的检测结果（比如人类），可以指定多个类别。 parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3') # 类别无关的 NMS（非极大值抑制），即忽略类别进行框抑制。默认按照类别进行 NMS。 parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS') # 使用增强的推理（例如镜像翻转）。默认不启用。 parser.add_argument('--augment', action='store_true', help='augmented inference') # 可视化模型特征图。默认不启用。 parser.add_argument('--visualize', action='store_true', help='visualize features') # 更新所有模型。默认不启用。 parser.add_argument('--update', action='store_true', help='update all models') # 设置项目保存的路径。默认保存到 `runs/detect/exp` 目录。 parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name') # 设置保存结果的名称。默认名称为 'exp'。 parser.add_argument('--name', default='exp', help='save results to project/name') # 允许覆盖已存在的项目文件夹。默认会创建一个新文件夹。 parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment') # 设置边界框的线条粗细，默认是 3 像素。 parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)') # 隐藏类别标签。默认显示标签。 parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels') # 隐藏置信度分数。默认显示置信度。 parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences') # 使用 FP16 半精度推理。默认使用全精度。 parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference') # 使用 OpenCV DNN 推理 ONNX 模型。默认不启用。 parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference') # 设置视频帧跳跃间隔，默认为 1，表示不跳帧。该参数可以用来控制处理每隔几帧进行一次推理。 parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride') opt = parser.parse_args() # 如果只提供了一个尺寸值，复制该值作为宽高值，确保尺寸为正方形。 opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1 # expand # 打印解析后的参数 print_args(vars(opt)) return opt 关键参数翻译总结： --weights: 模型权重文件路径或 Triton URL --source: 输入源（文件/目录/URL/屏幕/摄像头） --data: 数据集配置文件路径 --imgsz: 推理图像大小（高度，宽度） --conf-thres: 置信度阈值 --iou-thres: NMS 的 IoU 阈值 --max-det: 每张图像的最大检测数量 --device: 使用的设备（GPU 或 CPU） --view-img: 显示检测结果 --save-txt: 保存检测结果为文本文件 --save-conf: 在保存的文本中保存置信度值 --save-crop: 保存裁剪的预测框图像 --nosave: 不保存结果图像或视频 --classes: 通过类别过滤检测结果 --agnostic-nms: 类别无关的 NMS --augment: 增强推理 --visualize: 可视化特征图 --update: 更新所有模型 --project: 设置保存结果的项目路径 --name: 设置保存结果的文件夹名称 --exist-ok: 允许覆盖现有项目文件夹 --line-thickness: 边界框线条粗细 --hide-labels: 隐藏检测结果中的标签 --hide-conf: 隐藏置信度分数 --half: 使用 FP16 半精度推理 --dnn: 使用 OpenCV DNN 推理 ONNX 模型 --vid-stride: 视频帧跳跃间隔 置信度 ","link":"https://h-pl.github.io/post/xi-shuo-yolov5-de-detect-can-shu-yu-ming-ling-xing/"},{"title":"WSL下Docke内开始训练第一个yolov5s模型","content":"1.训练与环境 Windows下的WSL，内部安装wsl，在wsl安装ubuntu22.04。👍还是gpu快一些。 硬件： CPU：Intel i5-1135G7@2.4Ghz 内存：16G GPU：NVIDIA GeForce MX450 1.1tips 🤡镜像内部的Ubuntu系统和wsl的系统版本无关，是依据于镜像配置决定的。 2.使用GPU进行训练的流程 #启用gpu，映射硬盘，并进入docker #为防止BUS error，使用增加共享内存大小，进入docker，指定了内存大小， docker run --gpus all -it -v /mnt/d/3588:/mnt/3588 --shm-size=16g rknn3588:dev /bin/bash #使用train.py #根据自己的硬件条件调整，batch-size、workers python train.py --data mydata.yaml --weights runs/train/exp11/weights/best.pt --cfg yolov5s.yaml --batch-size 4 --epochs 100 --workers 2 #接着训练 python train.py --resume --weights runs/train/exp11/weights/last.pt #或手动接续训练，best.pt改成最近训练的pt python train.py --data mydata.yaml --weights runs/train/exp11/weights/best.pt --cfg yolov5s.yaml --batch-size 4 --epochs 100 --workers 2 2.1 训练结果 2.2 检测pt 多使用yolov5工具包自带的detect.py，可以接受图片和视频。详情可以看 # 检测多张图片并保存检测结果；可作为自动检测的工具 python detect.py --weights model1.pt --source path/to/images --save-txt #检测多张图片输出检测结果，其中检测结果中又置信度 python detect.py --weights model1.pt --source path/to/images --save-txt --save-conf #检测视频流，每隔150帧检测一次，保存检测结果 python detect.py --weights vestAndHelmet.pt --source testVideo/video.mp4 --save-txt --vid-stride 150 #检测视频流，每隔25帧检测一次，保存检测部位的裁剪图片 python detect.py --weights vestAndHelmet.pt --source testVideo/video.mp4 --save-txt --save-crop --vid-stride 25 《细说detect的参数》 3.关于硬件的一些问题 3.1. 数据加载器的内存问题（Bus Error） 从错误信息来看，出现了 Bus error，通常是由于 DataLoader 的工作线程用尽了共享内存。你可以通过调整数据加载器的设置来增加共享内存限制，并确保 GPU 正常工作。 解决方法： 你可以尝试增加 Docker 容器的共享内存（shm）大小，或调整 DataLoader 的工作线程数量。 增加共享内存： 在运行 Docker 容器时，可以使用 --shm-size 参数来增加共享内存大小。例如，增加到 16GB： docker run --gpus all -it -v /mnt/d/3588:/mnt/3588 --shm-size=16g rknn3588:dev /bin/bash 减少 DataLoader 的工作线程： 你可以减少 DataLoader 使用的工作线程数量，避免内存不足。将 workers 数量减少为 1 或 2，修改训练命令中的 --workers 参数： python train.py --data mydata.yaml --weights runs/train/exp11/weights/best.pt --cfg yolov5s.yaml --batch-size 4 --epochs 100 --workers 2 3.1. 硬件与batch-size、workers的设置 batch-size 和 workers 的设置不仅仅影响收 GPU 性能，还会受系统内存 (RAM) 和 GPU 内存 (VRAM) 的限制，尤其是在内存较小的系统中。内存往往成为训练的瓶颈。 如：我的硬件配置下，使用命令python train.py --data mydata.yaml --weights runs/train/exp11/weights/best.pt --cfg yolov5s.yaml --batch-size 4 --epochs 100 --workers 2后，硬件占用情况如下： 硬件 型号 占用 CPU Intel i5-1135G7@2.4Ghz 40%左右 GPUNVIDIA GeForce MX450 8%左右 内存 16G@3200Mhz 40%左右 3.1.硬件与 batch-size、workers 经验关系表 硬件配置 系统内存 GPU 内存 建议的 batch-size 建议的 workers 备注 低端 CPU (如 i5-1135G7) 8GB - 16GB 2GB - 4GB 1 - 4 1 - 2 内存瓶颈，batch-size 和 workers 都需控制，避免内存溢出。GPU 内存限制较小，workers 数量要少。 中端 CPU (如 i7 系列) 16GB - 32GB 4GB - 6GB 4 - 8 2 - 4 可处理较大的 batch-size 和 workers，但 GPU 内存限制仍然存在。适当增加 workers。 高端 CPU (如 i9 系列, AMD Ryzen 9) 32GB - 64GB 8GB - 16GB 8 - 16 4 - 8 内存和 GPU 都足够，可以处理较大的 batch-size 和更多的 workers，利用多线程加速数据加载。 入门级 GPU (如 MX450) 8GB - 16GB 2GB - 4GB 2 - 8 2 - 4 内存较小，限制 batch-size，需要减少 workers。较低的 GPU 内存影响 batch-size。 中端 GPU (如 GTX 1660 Ti, RTX 3050) 16GB - 32GB 6GB - 8GB 8 - 16 4 - 6 GPU 内存可支撑较大的 batch-size，增加 workers 以加速数据加载。内存仍有一定限制。 高端 GPU (如 RTX 3080, RTX 4090) 32GB - 64GB 10GB - 24GB 16 - 32 8 - 16 高内存和 GPU 内存，最大化 batch-size，并且可以使用更多的 workers。 多 GPU 环境 32GB - 128GB 8GB - 24GB (每个 GPU) 16 - 64 8 - 16 多卡训练时，每个 GPU 使用更大的 batch-size，workers 数量随之增加。 3.2.内存对 batch-size 和 workers 的影响： 系统内存 (RAM)： 高内存系统：当系统内存较大时，可以使用更多的 workers 和更大的 batch-size。在训练时，workers 用于加载数据并传递给模型，内存较大可以并行加载更多的数据。 低内存系统：当系统内存较小（例如 8GB 以下）时，增加 workers 会导致内存占用过高，可能引发系统性能下降或内存溢出。在这种情况下，应该限制 workers 数量，并使用较小的 batch-size。 GPU 内存 (VRAM)： 低 GPU 内存（如 2GB-4GB）时，batch-size 必须较小，否则 GPU 会因内存不足而导致 OOM (Out Of Memory) 错误。如果显存较小，应该尽量减少 batch-size，或者使用 half precision (FP16) 来减少显存占用。 较大 GPU 内存（如 8GB-16GB）可以处理更大的 batch-size。但是，如果显存不足，依然会导致 OOM 错误。在这种情况下，使用 gradient checkpointing 或减少 batch-size 仍然是有效的选择。 workers 的选择： 增加 workers 可以加速数据加载，特别是在使用 SSD 存储时。但是每增加一个 worker 就会占用额外的系统内存，因此在内存受限时，需要减少 workers 数量。 减少 workers 可以减轻内存压力，但可能导致数据加载成为训练的瓶颈，尤其是在硬盘读写速度较慢时。 3.3. 实际操作建议： 内存受限：将 batch-size 控制在较小值，workers 设置为 2 或更少，以防止内存占用过高。 显存受限：减少 batch-size 或尝试使用 half precision（FP16），可以减少 GPU 显存的占用。 内存和显存都足够：可以增加 batch-size 和 workers，尽可能提高训练效率。 3.4.如何调整训练参数： 如果内存占用过高，可以尝试： 降低 batch-size，例如从 8 调整为 4 或 2。 降低 workers，例如将 workers 从 4 调整为 2 或 1。 在训练时，使用较低分辨率的图像（例如将 640 降至 416 或 320）。 如果训练速度过慢，且内存和 GPU 显存足够，可以考虑： 增加 batch-size，例如从 4 提升到 8 或 16。 增加 workers，例如将 workers 从 2 增加到 4 或 6。 如何继续训练： 如果需要停掉当前训练并调整参数，确保你保存了训练的检查点（.pt 文件）。你可以通过命令 python train.py --resume --weights runs/train/exp11/weights/last.pt 来继续训练，而不会丢失已训练的进度。 这样，你可以根据硬件资源来动态调整训练的参数，以确保训练高效且稳定。 4.输出的参数 Epoch GPU_mem box_loss obj_loss cls_loss Instances Size 43/99 1.03G 0.04366 0.02842 0.00566 7 640: 100%|██████████| 55/55 00:33 Class Images Instances P R mAP50 mAP50-95: 100%|██████████| 5/5 00:01 all 40 131 0.618 0.486 0.496 0.26 在 YOLOv5 或类似模型的训练日志中，输出的各个参数代表了训练和验证阶段的不同指标。下面是对这些参数的详细解释，整理成表格形式： 4.1.训练过程中的输出参数解释 参数 说明 Epoch 当前训练的轮次（例如，第 43 轮 / 总 99 轮）。 GPU_mem 当前训练使用的 GPU 内存（例如 1.03GB）。表示在该轮次的训练中，GPU 的内存占用。 box_loss 边框损失（bounding box loss），表示模型预测的框与真实框之间的差异，通常使用 MSE 或 CIoU 等衡量。 obj_loss 目标损失（objectness loss），表示模型对物体存在与否的预测误差。 cls_loss 分类损失（classification loss），表示模型在物体分类方面的错误，通常使用交叉熵损失（cross-entropy）。 Instances 当前批次中使用的实例数（例如，7）。表示用于训练的图片数量。 Size 输入图片的尺寸（例如，640）。通常是输入图片的长或宽，模型的默认输入尺寸（如 640x640）表示每个图片的大小。 4.2.验证阶段输出的参数解释 参数 说明 Class 当前评估的类别（例如，all 表示所有类别的平均值）。 Images 当前验证集中的图像数量。 Instances 当前验证集中实例的总数。 P (Precision) 精度（Precision），表示预测为正类的样本中，真正正类的比例。公式：P = TP / (TP + FP)。 R (Recall) 召回率（Recall），表示真实正类中被正确预测为正类的比例。公式：R = TP / (TP + FN)。 mAP50 在 IoU 阈值为 0.5 时的平均精度（mean Average Precision at IoU=0.5）。 mAP50-95 在 IoU 阈值从 0.5 到 0.95 范围内（包括每个 0.05 步长）的平均精度（mAP at IoU=0.5:0.95）。 4.3.参数详细解释 Precision (P)：精度，衡量模型预测为正类的样本中，实际为正类的比例。高精度意味着较少的假阳性（FP）。 P ={TP}/{TP + FP} Recall (R)：召回率，衡量所有真实正类中，有多少被正确地预测为正类。高召回率意味着较少的假阴性（FN）。 R = {TP}/{TP + FN} mAP50：在 IoU 阈值为 0.5 时计算的平均精度，衡量在该阈值下模型的整体检测性能。通常是衡量目标检测性能的基本指标。 mAP50-95：在多个不同的 IoU 阈值下（从 0.5 到 0.95，每 0.05 步长）计算的平均精度。它更严格地评估了模型在不同重叠区域的检测精度。 TP (True Positives)：真正例，表示模型正确预测为正类的样本数。 FP (False Positives)：假正例，表示模型错误预测为正类的样本数。 FN (False Negatives)：假负例，表示模型错误预测为负类的样本数。 4.4.训练中的一些特定指标解释： box_loss, obj_loss, cls_loss：这些是反映训练中模型在不同任务上（定位、物体存在性、分类）的表现的损失值。较低的损失值意味着模型在这些任务上表现更好。 GPU_mem：GPU 内存的使用情况，较高的内存占用可能意味着 batch-size 或模型过大，可能需要优化。 4.5.总结： 训练阶段：box_loss、obj_loss 和 cls_loss 显示了模型在训练过程中不同任务的损失。 验证阶段：mAP50 和 mAP50-95 显示了模型的检测性能，而 P 和 R 是检验分类性能的关键指标。 希望这个表格能帮助你更好地理解训练日志中的参数，并优化你的训练过程！ 其他问题 1.为什么GPU没有使用？ 首先查看GPU的CUDA的驱动是否正确安装？然后查看Docker环境内的cuda驱动是否兼容？ #查看GPU驱动在WSL内 nvidia-smi #打印结果 Mon Nov 11 14:28:53 2024 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.183.04 Driver Version: 538.78 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA GeForce MX450 On | 00000000:05:00.0 Off | N/A | | N/A 51C P8 N/A / ERR! | 0MiB / 2048MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+ 2.1.没有使用gpu的原因 发现wsl的nvidia-container-toolkit没有安装。现在只要在docker的宿主机上，即wsl内安装nvidia-container-toolkit就可以了。 不需要像过去一样，安装支持nvidia的镜像了。sudo apt install nvidia-docker2 2.2.在wsl内安装支持docker的gpu驱动 #安装nvidia-container-toolkit apt-get update apt-get install -y nvidia-container-toolkit systemctl restart docker #检查nvidia驱动 nvidia-smi #使用python，检查cuda是否正常；在python交互命令行中 &gt;&gt;import torch &gt;&gt;print(torch.cuda.is_available()) TRUE # 看到true安装成功 2.3.查看docker内的cuda是否正常 #进入docker，开启gpu docker run --gpus all -it -v /mnt/d/3588:/mnt/3588 rknn3588:dev /bin/bash #使用python，检查cuda是否正常；在python交互命令行中 &gt;&gt;import torch &gt;&gt;print(torch.cuda.is_available()) TRUE # 看到true安装成功 3. 修改报错语句 train.py:307: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. with torch.cuda.amp.autocast(amp): # 一条弃用的api 要排查 train.py:307: FutureWarning: torch.cuda.amp.autocast(args...) is deprecated. Please use torch.amp.autocast('cuda', args...) instead 这个告警，建议按照以下步骤操作： 3.1. 更新代码以避免使用弃用的 API 告警提示 torch.cuda.amp.autocast 已经被弃用，建议使用 torch.amp.autocast('cuda', args...) 替代。你可以修改代码中相关部分，确保使用新 API。 修改前： with torch.cuda.amp.autocast(amp): # 训练代码 修改后： with torch.amp.autocast('cuda', enabled=amp): # 训练代码 这样可以消除警告，并且确保你使用的是 PyTorch 推荐的 API。 4.查看 TensorBoard 页面 我的环境是windows下的wsl内的docker，所以需要将docker的对应端口映射出来。 5.停止后接着上一次的训练 #训练模型，--batch-size会影响内存占用的大小 python train.py --data mydata.yaml --weight yolov5s.pt --cfg yolov5s.yaml --batch-size 2 --epochs 100 #接着上一次的训练 python train.py --data mydata.yaml --weights runs/train/exp8/weights/last.pt --cfg yolov5s.yaml --batch-size 2 --epochs 100 6.训练结果 Class Images Instances P R mAP50 mAP50-95: 100%|██████████| 12/12 00:03 all 47 267 0.88 0.532 0.613 0.308 Vest 47 113 0.844 0.867 0.902 0.487 helmet 47 115 0.786 0.809 0.826 0.379 noHelmet 47 21 1 0 0.172 0.0899 noVest 47 18 0.891 0.453 0.554 0.274 这是一个使用 YOLOv5 进行物体检测后的结果，显示了模型在不同类别上的检测性能。以下是对输出的解析和每个参数的说明： Class (类别): 模型检测到的对象类别，包括 Vest（安全背心）、helmet（头盔）、noHelmet（未戴头盔）、noVest（未穿背心）。 Images (图像): 用于验证模型的图像总数。这里是 47 张图像。 Instances (实例): 数据集中标注的对象实例总数。总共有 267 个实例分布在不同类别中。 P (Precision，精确率): 精确率衡量的是模型预测为正的样本中有多少是真正的正样本。精确率越高，说明模型对正类的预测更可靠。总的精确率是 0.88，其中 Vest 为 0.844，helmet 为 0.786，noHelmet 为 1，noVest 为 0.891。 R (Recall，召回率): 召回率衡量的是所有真实的正样本中，有多少被模型正确识别。召回率较低说明漏检率较高。整体的召回率是 0.532，其中 Vest 的召回率为 0.867，helmet 为 0.809，noHelmet 的召回率为 0，noVest 为 0.453。 mAP50 (mean Average Precision at IoU 0.5，平均精度均值，IoU = 0.5): mAP50 是在 IoU (Intersection over Union, 交并比) 为 0.5 时，计算每个类别的平均精度值（AP），并取均值。这个值衡量了模型在检测到目标位置时的准确性。整体 mAP50 为 0.613，其中 Vest 为 0.902，helmet 为 0.826，noHelmet 为 0.172，noVest 为 0.554。 mAP50-95 (mean Average Precision at IoU 0.5 to 0.95): mAP50-95 是更严格的评价标准，它计算在 IoU 范围从 0.5 到 0.95（步长为 0.05）下的平均精度。这个指标综合考虑了模型对目标检测的精度和位置的准确性。整体 mAP50-95 为 0.308，其中 Vest 为 0.487，helmet 为 0.379，noHelmet 为 0.0899，noVest 为 0.274。 解析与分析： 精确率（P）较高：模型对大多数类别预测为正时是可靠的，特别是 noHelmet，显示为 1（精确率非常高）。 召回率（R）偏低：整体召回率较低，说明模型漏检较多，尤其是 noHelmet 类别，召回率为 0，表示模型几乎未能识别未戴头盔的实例。 mAP50 高于 mAP50-95：mAP50 的值通常会高于 mAP50-95，因为在 IoU = 0.5 时，检测框的位置要求不如更高 IoU 时严格。从结果看，Vest 和 helmet 类别的检测较为理想，但对于未戴头盔（noHelmet）的检测效果较差。 优化建议： 提升召回率：可以考虑进一步优化训练数据，或者尝试调整模型参数来改善召回率，尤其是对难以检测的类别（如 noHelmet 和 noVest）。 调整阈值：可能需要调整检测阈值，尤其是针对 noHelmet 类别，以减少漏检。 ","link":"https://h-pl.github.io/post/wsl-xia-docke-nei-kai-shi-xun-lian-di-yi-ge-yolov5s-mo-xing/"},{"title":"WSL通过Docker安装RKNN-Toolkit2环境","content":"环境 我的环境是Windows下安装的WSL，系统内Ubuntu22.04。 1. 安装WSL Ubuntu22.04 1.1 启用或关闭Windows的功能，特别是要挂在到D盘下。 1.2 安装WSL， Microsoft AppStore 搜索 Ubuntu 22.04， 点击安装； 等待安装； 启动WSL，若启动异常，查看Ubuntu 22.04 的评论区。应是需更新wsl，wsl --update。 2.安装Docker 2.1 更换 apt 源为清华源。 sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak sudo nano /etc/apt/sources.list 注释或Ctrl+K 删除原来的官方源头。 Ubuntu22.04 代号 jammy，修改 sources.list 内容为： deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse 3.Docker的简单使用 #查看docker版本 docker --vesion #查看docker已安装的镜像images docker images ##打印镜像列表 REPOSITORY TAG IMAGE ID CREATED SIZE rknn-toolkit2 2.2.0-cp38 d371fba025c2 7 weeks ago 3.31GB hello-world latest d2c94e258dcb 18 months ago 13.3kB #进入docker的某一个镜像，启用交互终端进入容器的 shell。 docker run -it rknn-toolkit2:2.2.0-cp38 /bin/bash #以映射wsl的地址到docker内的方式，启用交互终端进入容器的 shell。 docker run -it -v /mnt/d/3588:/mnt/3588 rknn-toolkit2:2.2.0-cp38 /bin/bash #安装好的python环境容器持久化，将e95容器生成镜像rknn3588:dev docker commit e95 rknn3588:dev docker commit c46 rknn3588:used1107 #以映射wsl地址的方式进入rknn3588:dev镜像，启用交互终端进入容器的 shell。 docker run -it -v /mnt/d/3588:/mnt/3588 rknn3588:dev /bin/bash #docker内开启代理 root@ce60fd27a002:/mnt/3588/yolov5_export/yolov5_export# export https_proxy=http://10.16.168.39:10809 #显示docker所有镜像 docker image list #显示docker运行的容器 docker ps -a #进入docker正在运行的容器 docker exec -it 容器id前三位数字 /bin/bash #启动关闭的容器 docker start 容器id前三位数字 4.pip的简单使用 #查询pip的配置项 pip config list #更换pip源头 pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 5.rknn的转换模型与测试 #python 转换脚本 docker内 roadlight到onnx的模型 python export.py --rknpu --weight roadlight.pt #python 转换脚本3588板子 onnx到rknn的模型 python -m rknn.api.rknn_convert -t rk3588 -i ./model_config.yml -o ./ #python 测试图片 python test.py --img roadlight.jpg #python 测试视频、图片文件夹 python detect.py --weights vestAndHelmet.pt --source path/to/images --save-txt --save-conf 6.ffmpeg 视频抽帧 #每个180个关键帧取出一个关键帧 ffmpeg -i .\\21水-安全帽.mp4 -vf &quot;select='eq(pict_type,I)*not(mod(n\\,180))'&quot; -vsync vfr .\\21W\\output_%04d.jpg 7. python与python的虚拟环境 #创建项目路径 cd projectDir #创建虚拟环境 python -m venv yourEnvName #打开虚拟环境 yourEnvName\\Scripts\\activate #安装所需要依赖 python install -r requirements.txt #推出虚拟环境 deactivate 8.自动标注数据或验证模型 #使用yolov5的detect生成标注数据txt，不包含置信度 python detect.py --weights model1.pt model2.pt --source path/to/images --save-txt #包含置信度的命令，一般不用 python detect.py --weights model1.pt model2.pt --source path/to/images --save-txt --save-conf #添加classes.txt后，用labelImg二次编辑 #验证模型的准确性，150为帧数*5，为多少帧检测一次 python detect.py --weights vestAndHelmet.pt --source testVideo/video.mp4 --save-txt --vid-stride 150 9.使用lablestudio修改数据 #安装新的python虚拟环境labelstudio38 python -m venv labelstudio38 #安装labelstudio pip install label-studio #启动labelstudio label-studio start ","link":"https://h-pl.github.io/post/wsl-tong-guo-docker-an-zhuang-rknn-toolkit2-huan-jing/"},{"title":"奇怪的专业名词缩写","content":" 专业 缩写 原文 含义 备注 空调 A/S Air Condition/send 出风口 空调 A/R Air Condition/return 回风口 消防排烟 A/E Air Condition/exhaust [iɡˈzôst] 抽风口 多指抽烟口 ","link":"https://h-pl.github.io/post/tu-zhi-shang-de-zhuan-ye-ming-ci-suo-xie/"},{"title":"Dynamo 的神奇拓展包","content":"进入下面的正题之前，如果你安装的是revit2018，那么需要加装Dynamo[2.0.4]。其他版本的revit，请适配对应的最新版Dynamo。额外的，你还需要具备一些常用的 Dynamo 基础节点知识。 类目 节点名 作用 备注 Revit.Selection Select Model Element(Elements) 选择图元 交互式选择，鼠标点选或框选 Revit.Selection Element Types 选择项目中任意图元类型 枚举类下拉菜单 List.Modify List.FilterByBoolMask 按真假过滤列表 需和条件(codeblock)配合使用，如(x==&quot;...&quot;) Geometry.Geometry Geometry.Translate 将任意几何图形朝给定方向按给定距离平移 已知一个起点，平移得到端点 MEPover ","link":"https://h-pl.github.io/post/dynamo-de-shen-qi-tuo-zhan-bao/"},{"title":"Three.js 笔记","content":" three.js基础 ","link":"https://h-pl.github.io/post/threejs-bi-ji-mu-lu/"},{"title":"新概念Ⅱ中有趣的句子","content":"lesson 1-10 [lesson 4] My brother has never been aboard before, so he is finding this trip very exciting. 我弟弟以前从未出过国，所以他 认为 这次旅行非常 激动人心。 He is working for a big firm and he has already visited a great number of different places in Australia. 他就职于一家 大公司，并且去过澳大利亚不少地方。 ","link":"https://h-pl.github.io/post/xin-gai-nian-iizhong-you-qu-de-ju-zi/"},{"title":"评估 HVAC 系统在碳中和的优化方向","content":"关于这一议题，大多数已发表的研究都是基于假设和经验法。首次评估和制定了基于BIM的HVAC系统详细的全生命周期评估（LCA）的要求和方法。以瑞士大楼及现有的能效路径作为基础，LCA结果表明，暖通空调系统的影响是原来的三倍😲，且占总影响的15-36%。 研究目标 本文目标是：优化现有工作流程，因大量参数化工作，借助了可视化编程（VPL）将外部产品数据信息连接到BIM构件的方法。 建筑行业占全球碳排放的40% 高效的建筑技术和可再生能源代替传统能源 隐含碳是指建筑材料在制造、运输、施工和生命周期结束阶段释放的温室气体，占建筑行业的11% 在瑞士，暖通空调系统（供暖、通风、热分配）的制造和维护过程中的温室气体排放约占新办公建筑总排放量的13%😕 到目前为止，只有少数研究测量了HVAC系统的内在影响[17] 其中，只有一个[31]使用了一个建成的或几乎建成的BIM HVAC模型 一般来说，这类研究可大致分为两种类型的HVAC系统的LCA研究。 1️⃣第一种类型的研究在两个或更多的HVAC系统之间进行比较，以确定哪个系统对环境的影响更小，这些研究对详细的评估不感兴趣 2️⃣第二类HVAC的LCA是指对HVAC系统的详细评估📃，类似的研究有四项 其中，最具代表的一项研究发现 影响因素 影响占比 重要性 室内（天花板、门、家具、栏杆） 43% 最大 技术设备🔌 24% 结构框架 21% 围护结构 12% 研究方法 基于BIM的LCA被用于许多研究中，试图摆脱传统的LCA（基于手工的计算），并为设计者提供一个工具，以方便在设计阶段对建筑进行环境影响评估，以及在施工后进行评估（即符合标准或用于知识生成）。也有两种方法被大量使用，但它们🔨忽略了或过度简化了HVAC系统。在体现环境影响方面，以及在如何使用BIM模型和现有工具或方法评估暖通空调系统方面，存在着研究空白👀。 1️⃣第一种，目前主要借助于商业工具是LCA 与 BIM 相结合（如 Tally 或 OneClickLCA），主要原理是从BIM模型中提取信息（IFC或gbXML格式），然后导入LCA软件中 2️⃣第二种，使用BIM与可视化编程语言（VPL）。这种方法能够从BIM模型中自动提取信息，并创建可更新的链接到LCA数据库。 BIM模型提取的数据主要有几何数据、材料数据（数量和名称）和LCA数据。 几何信息是直接从BIM模型中提取的。 材料信息可以直接从BIM模型中提取。 在某些情况下，材料数量信息可以直接从BIM模型或产品数据表中获取。 在其他情况下，数量需要通过结合数学公式和来自BIM模型的几何信息来计算。 如果模型中没有这些信息，那么就用产品数据表来代替。 ####### 图1描述了HVAC系统的综合BIM和LCA工作流程。该示意图显示了与VPL相连的不同数据源。具体来说，来自BIM模型（1）、产品数据表（2）和LCA数据库（3）的信息在VPL环境中被结合起来，计算影响，并将结果以所需格式导出。本研究中使用的BIM软件是Revit 2019，而VPL是Dynamo 2.0。 从而实现以下功能🌈： 直接从BIM模型中提取物体和材料数据。 在BIM对象和产品数据表/目录之间以及材料和LCA数据库之间建立双向链接。 进行LCA计算并将结果导出到Excel文件中。 该工具的灵活性使其有可能根据所需的（LCA）边界条件、可用的数据以及数据格式和其结构来定制拟议的工作流程。 直接从BIM模型中提取物体和材料数据。 分析从对象层面开始，通过三个步骤将计算细化到材料层面。 1️⃣根据元素的复杂程度和性质，对元素进行高级分组和排序。管道与管道被归类在一起，因为它们都是线性元素，具有类似的细节（复杂性）。 2️⃣第二个层次，元素被进一步细分。例如，风管被细分为矩形风管和圆形风管。 3️⃣第三层也是最底层的分组/排序是根据材料类型进行的，例如，圆钢风管和圆铜风管。 根据共同特征，即几何形状和复杂程度，创建了四个不同的组。这四组是风管和管道，配件，机械设备和空气终端，以及管道和风管配件。总的来说，评估组的材料数量计算采用了三种方法。 1️⃣第一种方法中，从BIM模型中提取材料和几何信息，并利用科学公式（BIM数据与科学公式相结合）在Dynamo中计算材料的重量。 2️⃣第二种方法中，材料和几何信息从BIM模型中提取，在Dynamo中与产品数据表信息相结合（BIM数据与产品数据表数据相结合）。在大多数情况下，**来自产品数据表的重量信息是按物体而不是按材料提供的。**因此，必须假定👓各种物体材料在总重量中所占的百分比。 3️⃣第三种方法包括将物体与来自产品数据表的重量信息直接映射（BIM物体链接到产品数据表），也必须假设产品材料占总重量的百分比。 ⚠️如果上述的方法都不适用，比如配件的情况，那么就采用经验法则的估计方法 各类构件的计算方法 风管和管道的质量 两者统称为线性图元。可采用 其中，M是单位长度质量 kg/m，D是外径 mm，T是壁厚 mm 异形构件 实体对象，即无参数对象。计算直径与总面积的比值。根据面积比通过经验公式推导异形构件与总管道及风管的质量比。 机械设备与终端 来源于制造商 管道附件及新材料（组合材料） 来源于制造商 新材料（组合材料）来源于类似材料 LCA 数据来源 主要是由KBOB和Ecoinvent数据库检索而得。其中，LCA值主要基于材料映射。 案例分析 案例研究是位于瑞士的西门子国际总部办公大楼。该办公楼于 2018 年竣工，是位于楚格的西门子新园区的一部分。它拥有 LEED 白金认证和瑞士 Minergie 标签，重点关注建筑外壳和能源消耗。该建筑共有七层，总建筑面积（​​GFA）为 32,000 平方米，包括地下两层，主要用作车库。在暖通空调系统方面，该建筑以水为热泵运行，并使用楚格湖的水进行加热和自然冷却。这是首批使用 BIM 的西门子建筑项目之一。 HVAC 系统的 BIM 模型（图 2）在施工完成后经历了广泛的修订。它的开发水平 (LOD) 高于 300，尽管观察到某些元素比其他元素更详细。此外，该项目得到了很好的参数记录，包括大多数 HVAC 设备的产品数据表和详细信息。可以说，结合制造商信息的施工后BIM模型满足了竣工要求。 结论 材料数量提取结果表明，镀锌钢（66%）、铝（13%）和矿棉（10%）是主要材料。该案例钢材总量，包括镀锌钢、不锈钢、钢材，共计356吨，约占总材料量的80%（图3）。 当考虑到建筑的整个生命周期时，每个部件的LCA变得更加耐人寻味。与制造阶段相比，管道系统和机械设备的更换阶段产生的温室气体排放表明，与制造阶段的影响（15.3 kgCO2eq/m2）相比，机械设备在使用阶段的影响几乎翻倍（35 kgCO2eq/m2）。这种增加与建筑使用阶段的设备🌵更换频率有关。 例如，热泵是每20年更换一次。因此，在60年的建筑寿命中，它们被更换两次。材料数量的增加所产生的环境影响十分明显，两倍。总的来说，机械设备与风管和管道一起，是暖通空调系统生命周期内温室气体排放总量的主要贡献者。在更高的层面上，关于调查模块的结果显示，更换（B4）的年化排放量为1.70千克二氧化碳当量/平方米，是建筑生命周期阶段中碳密集度最高的；而制造（A1-A3）的年化排放量为1.32千克二氧化碳当量/平方米，以及运行（B6）的年化排放量为1.25千克二氧化碳当量/平方米，其重要性相近。运行影响的计算是基于公用事业公司提供的能源消耗数据，结合能源工程师提供的按计划的能源分配图。暖通空调系统的能源消耗占总能源消耗的57%。最后，弃置影响非常小（0.4 kgCO2eq/m2），主要来自于绝缘和辅助材料。 值得注意的是，大型空气处理机组AHU内的过滤器更换数量是😱相当可观的，占AHU更换总影响数的65%。而整个HVAC系统中，过滤器的更换占整个系统总更换影响的😱11%。其中，管道附件中，蝶阀，多叶风阀，流量控制器是碳排放量最大的。 参考文献 HVAC and BIM ","link":"https://h-pl.github.io/post/hvac-xi-tong-dui-huan-jing-de-ying-xiang/"},{"title":"半路出家学前端","content":" 原因。 我也不知道为什么是前端，这就叫执拗了。成功了一半，不是吗? 前端路线 大致的列表 HTML 基础 √ CSS 基础 √ ~~ 也可能忘记一半 ~~ JS学习笔记 ","link":"https://h-pl.github.io/post/ban-lu-chu-jia-xue-qian-duan/"}]}